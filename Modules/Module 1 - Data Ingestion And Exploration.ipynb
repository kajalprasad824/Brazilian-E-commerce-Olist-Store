{"cells": [{"cell_type": "code", "execution_count": 1, "id": "4228199c-2f9e-4f18-be99-f080a22e2754", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/04/12 15:54:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n.appName('OlistData') \\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "dc2eb71f-8d2e-4366-8a0a-f86a68eaaee6", "metadata": {"tags": []}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://cluster-e93b-m.us-central1-f.c.ace-ensign-453414-f0.internal:39237\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f7610b34cd0>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 3, "id": "6c0bd640-4df4-4a64-94b8-45659661fa93", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 9 items\n-rw-r--r--   2 klpd1299 hadoop    9033957 2025-04-12 15:51 /data/olist/olist_customers_dataset.csv\n-rw-r--r--   2 klpd1299 hadoop   61273883 2025-04-12 15:51 /data/olist/olist_geolocation_dataset.csv\n-rw-r--r--   2 klpd1299 hadoop   15438671 2025-04-12 15:51 /data/olist/olist_order_items_dataset.csv\n-rw-r--r--   2 klpd1299 hadoop    5777138 2025-04-12 15:51 /data/olist/olist_order_payments_dataset.csv\n-rw-r--r--   2 klpd1299 hadoop   14451670 2025-04-12 15:51 /data/olist/olist_order_reviews_dataset.csv\n-rw-r--r--   2 klpd1299 hadoop   17654914 2025-04-12 15:51 /data/olist/olist_orders_dataset.csv\n-rw-r--r--   2 klpd1299 hadoop    2379446 2025-04-12 15:51 /data/olist/olist_products_dataset.csv\n-rw-r--r--   2 klpd1299 hadoop     174703 2025-04-12 15:51 /data/olist/olist_sellers_dataset.csv\n-rw-r--r--   2 klpd1299 hadoop       2613 2025-04-12 15:51 /data/olist/product_category_name_translation.csv\n"}], "source": "!hadoop fs -ls /data/olist/"}, {"cell_type": "code", "execution_count": 4, "id": "a81e08bf-c079-45c5-ac0f-e9ade55c1629", "metadata": {"tags": []}, "outputs": [], "source": "hdfs_path = '/data/olist/'"}, {"cell_type": "code", "execution_count": 6, "id": "7ab45e04-6012-4c81-a330-0df4651c4ea2", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customer_df = spark.read.csv(hdfs_path + 'olist_customers_dataset.csv',header=True,inferSchema=True)"}, {"cell_type": "code", "execution_count": 7, "id": "2d7cd45b-bad8-44a0-9901-f347c35b2f10", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 2:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+------------------------+--------------------+--------------+\n|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n+--------------------+--------------------+------------------------+--------------------+--------------+\n|06b8999e2fba1a1fb...|861eff4711a542e4b...|                   14409|              franca|            SP|\n|18955e83d337fd6b2...|290c77bc529b7ac93...|                    9790|sao bernardo do c...|            SP|\n|4e7b3e00288586ebd...|060e732b5b29e8181...|                    1151|           sao paulo|            SP|\n|b2b6027bc5c5109e5...|259dac757896d24d7...|                    8775|     mogi das cruzes|            SP|\n|4f2d8ab171c80ec83...|345ecd01c38d18a90...|                   13056|            campinas|            SP|\n+--------------------+--------------------+------------------------+--------------------+--------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "customer_df.show(5)"}, {"cell_type": "code", "execution_count": 8, "id": "97c88aec-3a24-46ce-84d3-d5d5a5759a56", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "category_translation_df = spark.read.csv(hdfs_path + 'product_category_name_translation.csv',header=True,inferSchema=True)\norders_df = spark.read.csv(hdfs_path + 'olist_orders_dataset.csv',header=True,inferSchema=True)\norder_item_df = spark.read.csv(hdfs_path + 'olist_order_items_dataset.csv',header=True,inferSchema=True)\npayments_df = spark.read.csv(hdfs_path + 'olist_order_payments_dataset.csv',header=True,inferSchema=True)\nreviews_df = spark.read.csv(hdfs_path + 'olist_order_reviews_dataset.csv',header=True,inferSchema=True)\nproducts_df = spark.read.csv(hdfs_path + 'olist_products_dataset.csv',header=True,inferSchema=True)\nsellers_df = spark.read.csv(hdfs_path + 'olist_sellers_dataset.csv',header=True,inferSchema=True)\ngeolocation_df = spark.read.csv(hdfs_path + 'olist_geolocation_dataset.csv',header=True,inferSchema=True)"}, {"cell_type": "code", "execution_count": 10, "id": "635ba489-f454-452d-a6b3-49fa55a8d49e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: string (nullable = true)\n |-- customer_unique_id: string (nullable = true)\n |-- customer_zip_code_prefix: integer (nullable = true)\n |-- customer_city: string (nullable = true)\n |-- customer_state: string (nullable = true)\n\n"}], "source": "customer_df.printSchema()"}, {"cell_type": "code", "execution_count": 11, "id": "c7d8283d-f2c9-479a-9615-6a9f6435b146", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- order_status: string (nullable = true)\n |-- order_purchase_timestamp: timestamp (nullable = true)\n |-- order_approved_at: timestamp (nullable = true)\n |-- order_delivered_carrier_date: timestamp (nullable = true)\n |-- order_delivered_customer_date: timestamp (nullable = true)\n |-- order_estimated_delivery_date: timestamp (nullable = true)\n\n"}], "source": "orders_df.printSchema()"}, {"cell_type": "code", "execution_count": 12, "id": "48788022-b5ad-4148-9148-3e4bd99bc9de", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Customers : 99441 rows\nOrders : 99441 rows\n"}], "source": "# Data leakage or drop\nprint(f'Customers : {customer_df.count()} rows')\nprint(f'Orders : {orders_df.count()} rows')"}, {"cell_type": "code", "execution_count": 15, "id": "a8fc9060-fe11-4ef3-9e22-fe8cc8f09bfd", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+------------------+------------------------+-------------+--------------+\n|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|\n+-----------+------------------+------------------------+-------------+--------------+\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n|      false|             false|                   false|        false|         false|\n+-----------+------------------+------------------------+-------------+--------------+\nonly showing top 20 rows\n\n"}], "source": "# NULL or say duplicate values\n\nfrom pyspark.sql.functions import col\n\ncustomer_df.select([col(c).isNull().alias(c) for c in customer_df.columns]).show()"}, {"cell_type": "code", "execution_count": 16, "id": "e311f734-0697-4cca-a706-24642bb2a499", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+------------------+------------------------+-------------+--------------+\n|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|\n+-----------+------------------+------------------------+-------------+--------------+\n|          0|                 0|                       0|            0|             0|\n+-----------+------------------+------------------------+-------------+--------------+\n\n"}], "source": "# NULL or say duplicate values\n\nfrom pyspark.sql.functions import col,when,count\n\ncustomer_df.select([count(when(col(c).isNull(),1)).alias(c) for c in customer_df.columns]).show()"}, {"cell_type": "code", "execution_count": 17, "id": "9a879ccf-b9c4-4691-b361-90449401d2cc", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 29:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+-----+\n|customer_id|count|\n+-----------+-----+\n+-----------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Duplicate values\ncustomer_df.groupBy('customer_id').count().filter('count>1').show()"}, {"cell_type": "code", "execution_count": 19, "id": "52e8e10e-09e8-4fbc-9c6a-7c159b176587", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------+-----+\n|customer_state|count|\n+--------------+-----+\n|            RR|   46|\n|            AP|   68|\n|            AC|   81|\n|            AM|  148|\n|            RO|  253|\n|            TO|  280|\n|            SE|  350|\n|            AL|  413|\n|            RN|  485|\n|            PI|  495|\n|            PB|  536|\n|            MS|  715|\n|            MA|  747|\n|            MT|  907|\n|            PA|  975|\n|            CE| 1336|\n|            PE| 1652|\n|            GO| 2020|\n|            ES| 2033|\n|            DF| 2140|\n+--------------+-----+\nonly showing top 20 rows\n\n"}], "source": "## customer distribute by state\ncustomer_df.groupBy('customer_state').count().orderBy('count',ascending=True).show()"}, {"cell_type": "code", "execution_count": 21, "id": "f0db3b85-0eaa-40a1-9842-e402a7d3076c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+-----+\n|order_status|count|\n+------------+-----+\n|    approved|    2|\n|     created|    5|\n|  processing|  301|\n|    invoiced|  314|\n| unavailable|  609|\n|    canceled|  625|\n|     shipped| 1107|\n|   delivered|96478|\n+------------+-----+\n\n"}], "source": "orders_df.groupBy('order_status').count().orderBy('count',ascending=True).show()"}, {"cell_type": "code", "execution_count": 22, "id": "8a7303db-ae27-49ac-bbdb-9231a0cb39d6", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+-----+\n|payment_type|count|\n+------------+-----+\n| not_defined|    3|\n|  debit_card| 1529|\n|     voucher| 5775|\n|      boleto|19784|\n| credit_card|76795|\n+------------+-----+\n\n"}], "source": "payments_df.groupBy('payment_type').count().orderBy('count',ascending=True).show()"}, {"cell_type": "code", "execution_count": 23, "id": "0ecaf86d-2838-4f5e-889e-c7da2d9cd26a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 46:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------+\n|          product_id|       total_sales|\n+--------------------+------------------+\n|bb50f2e236e5eea01...|           63885.0|\n|6cdd53843498f9289...| 54730.20000000005|\n|d6160fb7873f18409...|48899.340000000004|\n|d1c427060a0f73f6b...| 47214.51000000006|\n|99a4788cb24856965...|43025.560000000085|\n|3dd2a17168ec895c7...| 41082.60000000005|\n|25c38557cf793876c...| 38907.32000000001|\n|5f504b3a1c75b73d6...|37733.899999999994|\n|53b36df67ebb7c415...| 37683.42000000001|\n|aca2eb7d00ea1a7b8...| 37608.90000000007|\n|e0d64dcfaa3b6db5c...|          31786.82|\n|d285360f29ac7fd97...|31623.809999999983|\n|7a10781637204d8d1...|           30467.5|\n|f1c7f353075ce59d8...|          29997.36|\n|f819f0c84a64f02d3...|29024.479999999996|\n|588531f8ec37e7d5f...|28291.989999999998|\n|422879e10f4668299...|26577.219999999972|\n|16c4e87b98a9370a9...|           25034.0|\n|5a848e4ab52fd5445...|24229.029999999962|\n|a62e25e09e05e6faf...|           24051.0|\n+--------------------+------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import sum\n\ntop_products = order_item_df.groupBy(\"product_id\").agg(sum('price').alias('total_sales'))\ntop_products.orderBy('total_sales',ascending=False).show(20)"}, {"cell_type": "code", "execution_count": 31, "id": "2717cd76-0216-4bc0-b813-760c64b74e17", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+------------------------+-----------------------------+\n|            order_id|order_purchase_timestamp|order_delivered_customer_date|\n+--------------------+------------------------+-----------------------------+\n|e481f51cbdc54678b...|     2017-10-02 10:56:33|          2017-10-10 21:25:13|\n|53cdb2fc8bc7dce0b...|     2018-07-24 20:41:37|          2018-08-07 15:27:45|\n|47770eb9100c2d0c4...|     2018-08-08 08:38:49|          2018-08-17 18:06:29|\n|949d5b44dbf5de918...|     2017-11-18 19:28:06|          2017-12-02 00:28:42|\n|ad21c59c0840e6cb8...|     2018-02-13 21:18:39|          2018-02-16 18:17:02|\n|a4591c265e18cb1dc...|     2017-07-09 21:57:05|          2017-07-26 10:57:55|\n|136cce7faa42fdb2c...|     2017-04-11 12:22:08|                         NULL|\n|6514b8ad8028c9f2c...|     2017-05-16 13:10:30|          2017-05-26 12:55:51|\n|76c6e866289321a7c...|     2017-01-23 18:29:09|          2017-02-02 14:08:10|\n|e69bfb5eb88e0ed6a...|     2017-07-29 11:55:02|          2017-08-16 17:14:30|\n|e6ce16cb79ec1d90b...|     2017-05-16 19:41:10|          2017-05-29 11:18:31|\n|34513ce0c4fab462a...|     2017-07-13 19:58:11|          2017-07-19 14:04:48|\n|82566a660a982b15f...|     2018-06-07 10:06:19|          2018-06-19 12:05:52|\n|5ff96c15d0b717ac6...|     2018-07-25 17:44:10|          2018-07-30 15:52:25|\n|432aaf21d85167c2c...|     2018-03-01 14:14:28|          2018-03-12 23:36:26|\n|dcb36b511fcac050b...|     2018-06-07 19:03:12|          2018-06-21 15:34:32|\n|403b97836b0c04a62...|     2018-01-02 19:00:43|          2018-01-20 01:38:59|\n|116f0b09343b49556...|     2017-12-26 23:41:31|          2018-01-08 22:36:36|\n|85ce859fd6dc634de...|     2017-11-21 00:03:41|          2017-11-27 18:28:00|\n|83018ec114eee8641...|     2017-10-26 15:54:26|          2017-11-08 22:22:00|\n+--------------------+------------------------+-----------------------------+\nonly showing top 20 rows\n\n"}], "source": "#Average Delivery Time Analysis\ndelivery_df = orders_df.select('order_id','order_purchase_timestamp','order_delivered_customer_date').show()"}, {"cell_type": "code", "execution_count": null, "id": "8bc5d80d-9bc7-4171-b37a-a4a21196109f", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}